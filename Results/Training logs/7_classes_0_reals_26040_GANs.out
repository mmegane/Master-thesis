nohup: ignoring input
2020-07-21 02:07:09.578626: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-07-21 02:07:09.593478: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3601810000 Hz
2020-07-21 02:07:09.595037: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc010882d0 executing computations on platform Host. Devices:
2020-07-21 02:07:09.595104: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-07-21 02:07:09.605542: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-07-21 02:07:09.635826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.637205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7715
pciBusID: 0000:01:00.0
2020-07-21 02:07:09.640460: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 02:07:09.649910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2020-07-21 02:07:09.657053: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2020-07-21 02:07:09.660462: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2020-07-21 02:07:09.668800: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2020-07-21 02:07:09.674060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2020-07-21 02:07:09.688469: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-07-21 02:07:09.688769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.690250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.691502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-07-21 02:07:09.691621: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 02:07:09.851676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-21 02:07:09.851767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-07-21 02:07:09.851803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-07-21 02:07:09.852194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.853638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.855017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.856456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7441 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-07-21 02:07:09.862316: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc01b93d10 executing computations on platform CUDA. Devices:
2020-07-21 02:07:09.862398: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
WARNING:tensorflow:From segmentation.py:20: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

WARNING:tensorflow:From segmentation.py:33: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From segmentation.py:35: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-07-21 02:07:09.865159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.866478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7715
pciBusID: 0000:01:00.0
2020-07-21 02:07:09.866599: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 02:07:09.866673: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2020-07-21 02:07:09.866736: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2020-07-21 02:07:09.866780: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2020-07-21 02:07:09.866823: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2020-07-21 02:07:09.866866: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2020-07-21 02:07:09.866911: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-07-21 02:07:09.867076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.868296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.869417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-07-21 02:07:09.869496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-21 02:07:09.869530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-07-21 02:07:09.869557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-07-21 02:07:09.869813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.871146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-21 02:07:09.872408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7441 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.

WARNING:tensorflow:From /home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /home/mehfo331/.conda/envs/thesis/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

2020-07-21 02:18:39.030840: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7

Reading data...
Finished reading data.

Reading data...
Finished reading data.

Training 7 classes with 26040 images (0 real and 26040 GAN images):

Epoch 1/150
 - 1978s - loss: 0.4072 - weighted_dice_loss: 0.4072 - val_loss: 0.4201 - val_weighted_dice_loss: 0.4201

Epoch 00001: val_weighted_dice_loss improved from inf to 0.42006, saving model to ./Unet-weights/7_classes_0_reals_26040_GANs.h5
Epoch 2/150
 - 1583s - loss: 0.2096 - weighted_dice_loss: 0.2096 - val_loss: 0.5574 - val_weighted_dice_loss: 0.5574

Epoch 00002: val_weighted_dice_loss did not improve from 0.42006
Epoch 3/150
 - 1581s - loss: 0.1803 - weighted_dice_loss: 0.1803 - val_loss: 0.4290 - val_weighted_dice_loss: 0.4290

Epoch 00003: val_weighted_dice_loss did not improve from 0.42006
Epoch 4/150
 - 1580s - loss: 0.1617 - weighted_dice_loss: 0.1617 - val_loss: 0.4490 - val_weighted_dice_loss: 0.4490

Epoch 00004: val_weighted_dice_loss did not improve from 0.42006
Epoch 5/150
 - 1582s - loss: 0.1497 - weighted_dice_loss: 0.1497 - val_loss: 0.4360 - val_weighted_dice_loss: 0.4360

Epoch 00005: val_weighted_dice_loss did not improve from 0.42006
Epoch 6/150
 - 1583s - loss: 0.1412 - weighted_dice_loss: 0.1412 - val_loss: 0.4667 - val_weighted_dice_loss: 0.4667

Epoch 00006: val_weighted_dice_loss did not improve from 0.42006
Epoch 7/150
 - 1579s - loss: 0.1346 - weighted_dice_loss: 0.1346 - val_loss: 0.4506 - val_weighted_dice_loss: 0.4506

Epoch 00007: val_weighted_dice_loss did not improve from 0.42006
Epoch 8/150
 - 1579s - loss: 0.1288 - weighted_dice_loss: 0.1288 - val_loss: 0.4511 - val_weighted_dice_loss: 0.4511

Epoch 00008: val_weighted_dice_loss did not improve from 0.42006
Epoch 9/150
 - 1580s - loss: 0.1227 - weighted_dice_loss: 0.1227 - val_loss: 0.4686 - val_weighted_dice_loss: 0.4686

Epoch 00009: val_weighted_dice_loss did not improve from 0.42006
Epoch 10/150
 - 1581s - loss: 0.1182 - weighted_dice_loss: 0.1182 - val_loss: 0.4764 - val_weighted_dice_loss: 0.4764

Epoch 00010: val_weighted_dice_loss did not improve from 0.42006
Epoch 11/150
 - 1580s - loss: 0.1139 - weighted_dice_loss: 0.1139 - val_loss: 0.4411 - val_weighted_dice_loss: 0.4411

Epoch 00011: val_weighted_dice_loss did not improve from 0.42006
Epoch 12/150
 - 1577s - loss: 0.1111 - weighted_dice_loss: 0.1111 - val_loss: 0.4670 - val_weighted_dice_loss: 0.4670

Epoch 00012: val_weighted_dice_loss did not improve from 0.42006
Epoch 13/150
 - 1576s - loss: 0.1071 - weighted_dice_loss: 0.1071 - val_loss: 0.4668 - val_weighted_dice_loss: 0.4668

Epoch 00013: val_weighted_dice_loss did not improve from 0.42006
Epoch 14/150
 - 1584s - loss: 0.1046 - weighted_dice_loss: 0.1046 - val_loss: 0.4700 - val_weighted_dice_loss: 0.4700

Epoch 00014: val_weighted_dice_loss did not improve from 0.42006
Epoch 15/150
 - 1581s - loss: 0.1009 - weighted_dice_loss: 0.1009 - val_loss: 0.4422 - val_weighted_dice_loss: 0.4422

Epoch 00015: val_weighted_dice_loss did not improve from 0.42006
Epoch 16/150
 - 1580s - loss: 0.0987 - weighted_dice_loss: 0.0987 - val_loss: 0.4579 - val_weighted_dice_loss: 0.4579

Epoch 00016: val_weighted_dice_loss did not improve from 0.42006
Epoch 17/150
 - 1581s - loss: 0.0963 - weighted_dice_loss: 0.0963 - val_loss: 0.4519 - val_weighted_dice_loss: 0.4519

Epoch 00017: val_weighted_dice_loss did not improve from 0.42006
Epoch 18/150
 - 1585s - loss: 0.0942 - weighted_dice_loss: 0.0942 - val_loss: 0.4478 - val_weighted_dice_loss: 0.4478

Epoch 00018: val_weighted_dice_loss did not improve from 0.42006
Epoch 19/150
 - 1580s - loss: 0.0928 - weighted_dice_loss: 0.0928 - val_loss: 0.4424 - val_weighted_dice_loss: 0.4424

Epoch 00019: val_weighted_dice_loss did not improve from 0.42006
Epoch 20/150
 - 1582s - loss: 0.0899 - weighted_dice_loss: 0.0899 - val_loss: 0.4334 - val_weighted_dice_loss: 0.4334

Epoch 00020: val_weighted_dice_loss did not improve from 0.42006
Epoch 21/150
 - 1582s - loss: 0.0886 - weighted_dice_loss: 0.0886 - val_loss: 0.4468 - val_weighted_dice_loss: 0.4468

Epoch 00021: val_weighted_dice_loss did not improve from 0.42006
Epoch 22/150
 - 1585s - loss: 0.0869 - weighted_dice_loss: 0.0869 - val_loss: 0.4408 - val_weighted_dice_loss: 0.4408

Epoch 00022: val_weighted_dice_loss did not improve from 0.42006
Epoch 23/150
 - 1591s - loss: 0.0855 - weighted_dice_loss: 0.0855 - val_loss: 0.4324 - val_weighted_dice_loss: 0.4324

Epoch 00023: val_weighted_dice_loss did not improve from 0.42006
Epoch 24/150
 - 1585s - loss: 0.0840 - weighted_dice_loss: 0.0840 - val_loss: 0.4579 - val_weighted_dice_loss: 0.4579

Epoch 00024: val_weighted_dice_loss did not improve from 0.42006
Epoch 25/150
 - 1582s - loss: 0.0827 - weighted_dice_loss: 0.0827 - val_loss: 0.4335 - val_weighted_dice_loss: 0.4335

Epoch 00025: val_weighted_dice_loss did not improve from 0.42006
Epoch 26/150
 - 1583s - loss: 0.0814 - weighted_dice_loss: 0.0814 - val_loss: 0.4498 - val_weighted_dice_loss: 0.4498

Epoch 00026: val_weighted_dice_loss did not improve from 0.42006
Epoch 27/150
 - 1583s - loss: 0.0801 - weighted_dice_loss: 0.0801 - val_loss: 0.4429 - val_weighted_dice_loss: 0.4429

Epoch 00027: val_weighted_dice_loss did not improve from 0.42006
Epoch 28/150
 - 1582s - loss: 0.0791 - weighted_dice_loss: 0.0791 - val_loss: 0.4412 - val_weighted_dice_loss: 0.4412

Epoch 00028: val_weighted_dice_loss did not improve from 0.42006
Epoch 29/150
 - 1582s - loss: 0.0779 - weighted_dice_loss: 0.0779 - val_loss: 0.4654 - val_weighted_dice_loss: 0.4654

Epoch 00029: val_weighted_dice_loss did not improve from 0.42006
Epoch 30/150
 - 1580s - loss: 0.0769 - weighted_dice_loss: 0.0769 - val_loss: 0.4596 - val_weighted_dice_loss: 0.4596

Epoch 00030: val_weighted_dice_loss did not improve from 0.42006
Epoch 31/150
 - 1581s - loss: 0.0759 - weighted_dice_loss: 0.0759 - val_loss: 0.4289 - val_weighted_dice_loss: 0.4289

Epoch 00031: val_weighted_dice_loss did not improve from 0.42006
Epoch 32/150
 - 1582s - loss: 0.0750 - weighted_dice_loss: 0.0750 - val_loss: 0.4527 - val_weighted_dice_loss: 0.4527

Epoch 00032: val_weighted_dice_loss did not improve from 0.42006
Epoch 33/150
 - 1585s - loss: 0.0740 - weighted_dice_loss: 0.0740 - val_loss: 0.4515 - val_weighted_dice_loss: 0.4515

Epoch 00033: val_weighted_dice_loss did not improve from 0.42006
Epoch 34/150
 - 1582s - loss: 0.0732 - weighted_dice_loss: 0.0732 - val_loss: 0.4523 - val_weighted_dice_loss: 0.4523

Epoch 00034: val_weighted_dice_loss did not improve from 0.42006
Epoch 35/150
 - 1582s - loss: 0.0724 - weighted_dice_loss: 0.0724 - val_loss: 0.4365 - val_weighted_dice_loss: 0.4365

Epoch 00035: val_weighted_dice_loss did not improve from 0.42006
Epoch 36/150
 - 1579s - loss: 0.0715 - weighted_dice_loss: 0.0715 - val_loss: 0.4493 - val_weighted_dice_loss: 0.4493

Epoch 00036: val_weighted_dice_loss did not improve from 0.42006
Epoch 37/150
 - 1581s - loss: 0.0707 - weighted_dice_loss: 0.0707 - val_loss: 0.4476 - val_weighted_dice_loss: 0.4476

Epoch 00037: val_weighted_dice_loss did not improve from 0.42006
Epoch 38/150
 - 1586s - loss: 0.0701 - weighted_dice_loss: 0.0701 - val_loss: 0.4322 - val_weighted_dice_loss: 0.4322

Epoch 00038: val_weighted_dice_loss did not improve from 0.42006
Epoch 39/150
 - 1586s - loss: 0.0694 - weighted_dice_loss: 0.0694 - val_loss: 0.4384 - val_weighted_dice_loss: 0.4384

Epoch 00039: val_weighted_dice_loss did not improve from 0.42006
Epoch 40/150
 - 1580s - loss: 0.0687 - weighted_dice_loss: 0.0687 - val_loss: 0.4455 - val_weighted_dice_loss: 0.4455

Epoch 00040: val_weighted_dice_loss did not improve from 0.42006
Epoch 41/150
 - 1578s - loss: 0.0680 - weighted_dice_loss: 0.0680 - val_loss: 0.4537 - val_weighted_dice_loss: 0.4537

Epoch 00041: val_weighted_dice_loss did not improve from 0.42006
Epoch 42/150
 - 1582s - loss: 0.0676 - weighted_dice_loss: 0.0676 - val_loss: 0.4403 - val_weighted_dice_loss: 0.4403

Epoch 00042: val_weighted_dice_loss did not improve from 0.42006
Epoch 43/150
 - 1583s - loss: 0.0668 - weighted_dice_loss: 0.0668 - val_loss: 0.4376 - val_weighted_dice_loss: 0.4376

Epoch 00043: val_weighted_dice_loss did not improve from 0.42006
Epoch 44/150
 - 1582s - loss: 0.0663 - weighted_dice_loss: 0.0663 - val_loss: 0.4544 - val_weighted_dice_loss: 0.4544

Epoch 00044: val_weighted_dice_loss did not improve from 0.42006
Epoch 45/150
 - 1579s - loss: 0.0657 - weighted_dice_loss: 0.0657 - val_loss: 0.4499 - val_weighted_dice_loss: 0.4499

Epoch 00045: val_weighted_dice_loss did not improve from 0.42006
Epoch 46/150
 - 1580s - loss: 0.0651 - weighted_dice_loss: 0.0651 - val_loss: 0.4359 - val_weighted_dice_loss: 0.4359

Epoch 00046: val_weighted_dice_loss did not improve from 0.42006
Epoch 47/150
 - 1581s - loss: 0.0647 - weighted_dice_loss: 0.0647 - val_loss: 0.4544 - val_weighted_dice_loss: 0.4544

Epoch 00047: val_weighted_dice_loss did not improve from 0.42006
Epoch 48/150
 - 1579s - loss: 0.0640 - weighted_dice_loss: 0.0640 - val_loss: 0.4345 - val_weighted_dice_loss: 0.4345

Epoch 00048: val_weighted_dice_loss did not improve from 0.42006
Epoch 49/150
 - 1580s - loss: 0.0635 - weighted_dice_loss: 0.0635 - val_loss: 0.4609 - val_weighted_dice_loss: 0.4609

Epoch 00049: val_weighted_dice_loss did not improve from 0.42006
Epoch 50/150
 - 1581s - loss: 0.0631 - weighted_dice_loss: 0.0631 - val_loss: 0.4443 - val_weighted_dice_loss: 0.4443

Epoch 00050: val_weighted_dice_loss did not improve from 0.42006
Epoch 51/150
 - 1583s - loss: 0.0625 - weighted_dice_loss: 0.0625 - val_loss: 0.4551 - val_weighted_dice_loss: 0.4551

Epoch 00051: val_weighted_dice_loss did not improve from 0.42006
Epoch 52/150
 - 1582s - loss: 0.0621 - weighted_dice_loss: 0.0621 - val_loss: 0.4460 - val_weighted_dice_loss: 0.4460

Epoch 00052: val_weighted_dice_loss did not improve from 0.42006
Epoch 53/150
 - 1581s - loss: 0.0618 - weighted_dice_loss: 0.0618 - val_loss: 0.4432 - val_weighted_dice_loss: 0.4432

Epoch 00053: val_weighted_dice_loss did not improve from 0.42006
Epoch 54/150
 - 1576s - loss: 0.0612 - weighted_dice_loss: 0.0612 - val_loss: 0.4467 - val_weighted_dice_loss: 0.4467

Epoch 00054: val_weighted_dice_loss did not improve from 0.42006
Epoch 55/150
 - 1576s - loss: 0.0607 - weighted_dice_loss: 0.0607 - val_loss: 0.4329 - val_weighted_dice_loss: 0.4329

Epoch 00055: val_weighted_dice_loss did not improve from 0.42006
Epoch 56/150
 - 1578s - loss: 0.0603 - weighted_dice_loss: 0.0603 - val_loss: 0.4412 - val_weighted_dice_loss: 0.4412

Epoch 00056: val_weighted_dice_loss did not improve from 0.42006
Epoch 57/150
 - 1579s - loss: 0.0598 - weighted_dice_loss: 0.0598 - val_loss: 0.4496 - val_weighted_dice_loss: 0.4496

Epoch 00057: val_weighted_dice_loss did not improve from 0.42006
Epoch 58/150
 - 1579s - loss: 0.0594 - weighted_dice_loss: 0.0594 - val_loss: 0.4263 - val_weighted_dice_loss: 0.4263

Epoch 00058: val_weighted_dice_loss did not improve from 0.42006
Epoch 59/150
 - 1580s - loss: 0.0591 - weighted_dice_loss: 0.0591 - val_loss: 0.4408 - val_weighted_dice_loss: 0.4408

Epoch 00059: val_weighted_dice_loss did not improve from 0.42006
Epoch 60/150
 - 1583s - loss: 0.0587 - weighted_dice_loss: 0.0587 - val_loss: 0.4285 - val_weighted_dice_loss: 0.4285

Epoch 00060: val_weighted_dice_loss did not improve from 0.42006
Epoch 61/150
 - 1584s - loss: 0.0582 - weighted_dice_loss: 0.0582 - val_loss: 0.4223 - val_weighted_dice_loss: 0.4223

Epoch 00061: val_weighted_dice_loss did not improve from 0.42006
Epoch 62/150
 - 1577s - loss: 0.0580 - weighted_dice_loss: 0.0580 - val_loss: 0.4394 - val_weighted_dice_loss: 0.4394

Epoch 00062: val_weighted_dice_loss did not improve from 0.42006
Epoch 63/150
 - 1590s - loss: 0.0576 - weighted_dice_loss: 0.0576 - val_loss: 0.4399 - val_weighted_dice_loss: 0.4399

Epoch 00063: val_weighted_dice_loss did not improve from 0.42006
Epoch 64/150
 - 1584s - loss: 0.0572 - weighted_dice_loss: 0.0572 - val_loss: 0.4399 - val_weighted_dice_loss: 0.4399

Epoch 00064: val_weighted_dice_loss did not improve from 0.42006
Epoch 65/150
 - 1580s - loss: 0.0569 - weighted_dice_loss: 0.0569 - val_loss: 0.4515 - val_weighted_dice_loss: 0.4515

Epoch 00065: val_weighted_dice_loss did not improve from 0.42006
Epoch 66/150
 - 1581s - loss: 0.0566 - weighted_dice_loss: 0.0566 - val_loss: 0.4454 - val_weighted_dice_loss: 0.4454

Epoch 00066: val_weighted_dice_loss did not improve from 0.42006
Epoch 67/150
 - 1580s - loss: 0.0562 - weighted_dice_loss: 0.0562 - val_loss: 0.4423 - val_weighted_dice_loss: 0.4423

Epoch 00067: val_weighted_dice_loss did not improve from 0.42006
Epoch 68/150
 - 1582s - loss: 0.0558 - weighted_dice_loss: 0.0558 - val_loss: 0.4455 - val_weighted_dice_loss: 0.4455

Epoch 00068: val_weighted_dice_loss did not improve from 0.42006
Epoch 69/150
 - 1580s - loss: 0.0556 - weighted_dice_loss: 0.0556 - val_loss: 0.4446 - val_weighted_dice_loss: 0.4446

Epoch 00069: val_weighted_dice_loss did not improve from 0.42006
Epoch 70/150
 - 1584s - loss: 0.0552 - weighted_dice_loss: 0.0552 - val_loss: 0.4393 - val_weighted_dice_loss: 0.4393

Epoch 00070: val_weighted_dice_loss did not improve from 0.42006
Epoch 71/150
 - 1578s - loss: 0.0549 - weighted_dice_loss: 0.0549 - val_loss: 0.4435 - val_weighted_dice_loss: 0.4435

Epoch 00071: val_weighted_dice_loss did not improve from 0.42006
Epoch 72/150
 - 1581s - loss: 0.0544 - weighted_dice_loss: 0.0544 - val_loss: 0.4403 - val_weighted_dice_loss: 0.4403

Epoch 00072: val_weighted_dice_loss did not improve from 0.42006
Epoch 73/150
 - 1583s - loss: 0.0543 - weighted_dice_loss: 0.0543 - val_loss: 0.4397 - val_weighted_dice_loss: 0.4397

Epoch 00073: val_weighted_dice_loss did not improve from 0.42006
Epoch 74/150
 - 1581s - loss: 0.0540 - weighted_dice_loss: 0.0540 - val_loss: 0.4430 - val_weighted_dice_loss: 0.4430

Epoch 00074: val_weighted_dice_loss did not improve from 0.42006
Epoch 75/150
 - 1581s - loss: 0.0536 - weighted_dice_loss: 0.0536 - val_loss: 0.4542 - val_weighted_dice_loss: 0.4542

Epoch 00075: val_weighted_dice_loss did not improve from 0.42006
Epoch 76/150
 - 1588s - loss: 0.0533 - weighted_dice_loss: 0.0533 - val_loss: 0.4499 - val_weighted_dice_loss: 0.4499

Epoch 00076: val_weighted_dice_loss did not improve from 0.42006
Epoch 77/150
 - 1580s - loss: 0.0531 - weighted_dice_loss: 0.0531 - val_loss: 0.4417 - val_weighted_dice_loss: 0.4417

Epoch 00077: val_weighted_dice_loss did not improve from 0.42006
Epoch 78/150
 - 1582s - loss: 0.0527 - weighted_dice_loss: 0.0527 - val_loss: 0.4564 - val_weighted_dice_loss: 0.4564

Epoch 00078: val_weighted_dice_loss did not improve from 0.42006
Epoch 79/150
 - 1571s - loss: 0.0525 - weighted_dice_loss: 0.0525 - val_loss: 0.4503 - val_weighted_dice_loss: 0.4503

Epoch 00079: val_weighted_dice_loss did not improve from 0.42006
Epoch 80/150
 - 1578s - loss: 0.0524 - weighted_dice_loss: 0.0524 - val_loss: 0.4425 - val_weighted_dice_loss: 0.4425

Epoch 00080: val_weighted_dice_loss did not improve from 0.42006
Epoch 81/150
 - 1578s - loss: 0.0520 - weighted_dice_loss: 0.0520 - val_loss: 0.4460 - val_weighted_dice_loss: 0.4460

Epoch 00081: val_weighted_dice_loss did not improve from 0.42006
Epoch 82/150
 - 1584s - loss: 0.0517 - weighted_dice_loss: 0.0517 - val_loss: 0.4456 - val_weighted_dice_loss: 0.4456

Epoch 00082: val_weighted_dice_loss did not improve from 0.42006
Epoch 83/150
 - 1580s - loss: 0.0514 - weighted_dice_loss: 0.0514 - val_loss: 0.4382 - val_weighted_dice_loss: 0.4382

Epoch 00083: val_weighted_dice_loss did not improve from 0.42006
Epoch 84/150
 - 1585s - loss: 0.0511 - weighted_dice_loss: 0.0511 - val_loss: 0.4515 - val_weighted_dice_loss: 0.4515

Epoch 00084: val_weighted_dice_loss did not improve from 0.42006
Epoch 85/150
 - 1584s - loss: 0.0508 - weighted_dice_loss: 0.0508 - val_loss: 0.4469 - val_weighted_dice_loss: 0.4469

Epoch 00085: val_weighted_dice_loss did not improve from 0.42006
Epoch 86/150
 - 1590s - loss: 0.0507 - weighted_dice_loss: 0.0507 - val_loss: 0.4536 - val_weighted_dice_loss: 0.4536

Epoch 00086: val_weighted_dice_loss did not improve from 0.42006
Epoch 87/150
 - 1583s - loss: 0.0504 - weighted_dice_loss: 0.0504 - val_loss: 0.4474 - val_weighted_dice_loss: 0.4474

Epoch 00087: val_weighted_dice_loss did not improve from 0.42006
Epoch 88/150
 - 1588s - loss: 0.0502 - weighted_dice_loss: 0.0502 - val_loss: 0.4523 - val_weighted_dice_loss: 0.4523

Epoch 00088: val_weighted_dice_loss did not improve from 0.42006
Epoch 89/150
 - 1593s - loss: 0.0500 - weighted_dice_loss: 0.0500 - val_loss: 0.4419 - val_weighted_dice_loss: 0.4419

Epoch 00089: val_weighted_dice_loss did not improve from 0.42006
Epoch 90/150
 - 1586s - loss: 0.0498 - weighted_dice_loss: 0.0498 - val_loss: 0.4408 - val_weighted_dice_loss: 0.4408

Epoch 00090: val_weighted_dice_loss did not improve from 0.42006
Epoch 91/150
 - 1586s - loss: 0.0495 - weighted_dice_loss: 0.0495 - val_loss: 0.4465 - val_weighted_dice_loss: 0.4465

Epoch 00091: val_weighted_dice_loss did not improve from 0.42006
Epoch 92/150
 - 1581s - loss: 0.0493 - weighted_dice_loss: 0.0493 - val_loss: 0.4435 - val_weighted_dice_loss: 0.4435

Epoch 00092: val_weighted_dice_loss did not improve from 0.42006
Epoch 93/150
 - 1581s - loss: 0.0491 - weighted_dice_loss: 0.0491 - val_loss: 0.4495 - val_weighted_dice_loss: 0.4495

Epoch 00093: val_weighted_dice_loss did not improve from 0.42006
Epoch 94/150
 - 1585s - loss: 0.0488 - weighted_dice_loss: 0.0488 - val_loss: 0.4457 - val_weighted_dice_loss: 0.4457

Epoch 00094: val_weighted_dice_loss did not improve from 0.42006
Epoch 95/150
 - 1585s - loss: 0.0486 - weighted_dice_loss: 0.0486 - val_loss: 0.4421 - val_weighted_dice_loss: 0.4421

Epoch 00095: val_weighted_dice_loss did not improve from 0.42006
Epoch 96/150
 - 1583s - loss: 0.0484 - weighted_dice_loss: 0.0484 - val_loss: 0.4421 - val_weighted_dice_loss: 0.4421

Epoch 00096: val_weighted_dice_loss did not improve from 0.42006
Epoch 97/150
 - 1587s - loss: 0.0482 - weighted_dice_loss: 0.0482 - val_loss: 0.4333 - val_weighted_dice_loss: 0.4333

Epoch 00097: val_weighted_dice_loss did not improve from 0.42006
Epoch 98/150
 - 1583s - loss: 0.0480 - weighted_dice_loss: 0.0480 - val_loss: 0.4414 - val_weighted_dice_loss: 0.4414

Epoch 00098: val_weighted_dice_loss did not improve from 0.42006
Epoch 99/150
 - 1581s - loss: 0.0478 - weighted_dice_loss: 0.0478 - val_loss: 0.4502 - val_weighted_dice_loss: 0.4502

Epoch 00099: val_weighted_dice_loss did not improve from 0.42006
Epoch 100/150
 - 1586s - loss: 0.0476 - weighted_dice_loss: 0.0476 - val_loss: 0.4496 - val_weighted_dice_loss: 0.4496

Epoch 00100: val_weighted_dice_loss did not improve from 0.42006
Epoch 101/150
 - 1583s - loss: 0.0474 - weighted_dice_loss: 0.0474 - val_loss: 0.4388 - val_weighted_dice_loss: 0.4388

Epoch 00101: val_weighted_dice_loss did not improve from 0.42006
Epoch 102/150
 - 1584s - loss: 0.0472 - weighted_dice_loss: 0.0472 - val_loss: 0.4460 - val_weighted_dice_loss: 0.4460

Epoch 00102: val_weighted_dice_loss did not improve from 0.42006
Epoch 103/150
 - 1578s - loss: 0.0470 - weighted_dice_loss: 0.0470 - val_loss: 0.4290 - val_weighted_dice_loss: 0.4290

Epoch 00103: val_weighted_dice_loss did not improve from 0.42006
Epoch 104/150
 - 1586s - loss: 0.0467 - weighted_dice_loss: 0.0467 - val_loss: 0.4401 - val_weighted_dice_loss: 0.4401

Epoch 00104: val_weighted_dice_loss did not improve from 0.42006
Epoch 105/150
 - 1583s - loss: 0.0466 - weighted_dice_loss: 0.0466 - val_loss: 0.4483 - val_weighted_dice_loss: 0.4483

Epoch 00105: val_weighted_dice_loss did not improve from 0.42006
Epoch 106/150
 - 1579s - loss: 0.0465 - weighted_dice_loss: 0.0465 - val_loss: 0.4461 - val_weighted_dice_loss: 0.4461

Epoch 00106: val_weighted_dice_loss did not improve from 0.42006
Epoch 107/150
 - 1579s - loss: 0.0462 - weighted_dice_loss: 0.0462 - val_loss: 0.4509 - val_weighted_dice_loss: 0.4509

Epoch 00107: val_weighted_dice_loss did not improve from 0.42006
Epoch 108/150
 - 1583s - loss: 0.0461 - weighted_dice_loss: 0.0461 - val_loss: 0.4417 - val_weighted_dice_loss: 0.4417

Epoch 00108: val_weighted_dice_loss did not improve from 0.42006
Epoch 109/150
 - 1576s - loss: 0.0458 - weighted_dice_loss: 0.0458 - val_loss: 0.4410 - val_weighted_dice_loss: 0.4410

Epoch 00109: val_weighted_dice_loss did not improve from 0.42006
Epoch 110/150
 - 1580s - loss: 0.0457 - weighted_dice_loss: 0.0457 - val_loss: 0.4593 - val_weighted_dice_loss: 0.4593

Epoch 00110: val_weighted_dice_loss did not improve from 0.42006
Epoch 111/150
 - 1580s - loss: 0.0455 - weighted_dice_loss: 0.0455 - val_loss: 0.4349 - val_weighted_dice_loss: 0.4349

Epoch 00111: val_weighted_dice_loss did not improve from 0.42006
Epoch 112/150
 - 1578s - loss: 0.0453 - weighted_dice_loss: 0.0453 - val_loss: 0.4498 - val_weighted_dice_loss: 0.4498

Epoch 00112: val_weighted_dice_loss did not improve from 0.42006
Epoch 113/150
 - 1579s - loss: 0.0451 - weighted_dice_loss: 0.0451 - val_loss: 0.4505 - val_weighted_dice_loss: 0.4505

Epoch 00113: val_weighted_dice_loss did not improve from 0.42006
Epoch 114/150
 - 1583s - loss: 0.0449 - weighted_dice_loss: 0.0449 - val_loss: 0.4515 - val_weighted_dice_loss: 0.4515

Epoch 00114: val_weighted_dice_loss did not improve from 0.42006
Epoch 115/150
 - 1580s - loss: 0.0448 - weighted_dice_loss: 0.0448 - val_loss: 0.4377 - val_weighted_dice_loss: 0.4377

Epoch 00115: val_weighted_dice_loss did not improve from 0.42006
Epoch 116/150
 - 1577s - loss: 0.0446 - weighted_dice_loss: 0.0446 - val_loss: 0.4487 - val_weighted_dice_loss: 0.4487

Epoch 00116: val_weighted_dice_loss did not improve from 0.42006
Epoch 117/150
 - 1579s - loss: 0.0445 - weighted_dice_loss: 0.0445 - val_loss: 0.4605 - val_weighted_dice_loss: 0.4605

Epoch 00117: val_weighted_dice_loss did not improve from 0.42006
Epoch 118/150
 - 1585s - loss: 0.0444 - weighted_dice_loss: 0.0444 - val_loss: 0.4462 - val_weighted_dice_loss: 0.4462

Epoch 00118: val_weighted_dice_loss did not improve from 0.42006
Epoch 119/150
 - 1589s - loss: 0.0441 - weighted_dice_loss: 0.0441 - val_loss: 0.4521 - val_weighted_dice_loss: 0.4521

Epoch 00119: val_weighted_dice_loss did not improve from 0.42006
Epoch 120/150
 - 1583s - loss: 0.0440 - weighted_dice_loss: 0.0440 - val_loss: 0.4411 - val_weighted_dice_loss: 0.4411

Epoch 00120: val_weighted_dice_loss did not improve from 0.42006
Epoch 121/150
 - 1582s - loss: 0.0438 - weighted_dice_loss: 0.0438 - val_loss: 0.4508 - val_weighted_dice_loss: 0.4508

Epoch 00121: val_weighted_dice_loss did not improve from 0.42006
Epoch 122/150
 - 1581s - loss: 0.0436 - weighted_dice_loss: 0.0436 - val_loss: 0.4548 - val_weighted_dice_loss: 0.4548

Epoch 00122: val_weighted_dice_loss did not improve from 0.42006
Epoch 123/150
 - 1581s - loss: 0.0435 - weighted_dice_loss: 0.0435 - val_loss: 0.4371 - val_weighted_dice_loss: 0.4371

Epoch 00123: val_weighted_dice_loss did not improve from 0.42006
Epoch 124/150
 - 1578s - loss: 0.0434 - weighted_dice_loss: 0.0434 - val_loss: 0.4546 - val_weighted_dice_loss: 0.4546

Epoch 00124: val_weighted_dice_loss did not improve from 0.42006
Epoch 125/150
 - 1585s - loss: 0.0432 - weighted_dice_loss: 0.0432 - val_loss: 0.4393 - val_weighted_dice_loss: 0.4393

Epoch 00125: val_weighted_dice_loss did not improve from 0.42006
Epoch 126/150
 - 1580s - loss: 0.0429 - weighted_dice_loss: 0.0429 - val_loss: 0.4530 - val_weighted_dice_loss: 0.4530

Epoch 00126: val_weighted_dice_loss did not improve from 0.42006
Epoch 127/150
 - 1583s - loss: 0.0428 - weighted_dice_loss: 0.0428 - val_loss: 0.4455 - val_weighted_dice_loss: 0.4455

Epoch 00127: val_weighted_dice_loss did not improve from 0.42006
Epoch 128/150
 - 1581s - loss: 0.0426 - weighted_dice_loss: 0.0426 - val_loss: 0.4654 - val_weighted_dice_loss: 0.4654

Epoch 00128: val_weighted_dice_loss did not improve from 0.42006
Epoch 129/150
 - 1585s - loss: 0.0425 - weighted_dice_loss: 0.0425 - val_loss: 0.4434 - val_weighted_dice_loss: 0.4434

Epoch 00129: val_weighted_dice_loss did not improve from 0.42006
Epoch 130/150
 - 1581s - loss: 0.0425 - weighted_dice_loss: 0.0425 - val_loss: 0.4553 - val_weighted_dice_loss: 0.4553

Epoch 00130: val_weighted_dice_loss did not improve from 0.42006
Epoch 131/150
 - 1582s - loss: 0.0423 - weighted_dice_loss: 0.0423 - val_loss: 0.4386 - val_weighted_dice_loss: 0.4386

Epoch 00131: val_weighted_dice_loss did not improve from 0.42006
Epoch 132/150
 - 1584s - loss: 0.0422 - weighted_dice_loss: 0.0422 - val_loss: 0.4431 - val_weighted_dice_loss: 0.4431

Epoch 00132: val_weighted_dice_loss did not improve from 0.42006
Epoch 133/150
 - 1587s - loss: 0.0421 - weighted_dice_loss: 0.0421 - val_loss: 0.4534 - val_weighted_dice_loss: 0.4534

Epoch 00133: val_weighted_dice_loss did not improve from 0.42006
Epoch 134/150
 - 1594s - loss: 0.0419 - weighted_dice_loss: 0.0419 - val_loss: 0.4398 - val_weighted_dice_loss: 0.4398

Epoch 00134: val_weighted_dice_loss did not improve from 0.42006
Epoch 135/150
 - 1580s - loss: 0.0417 - weighted_dice_loss: 0.0417 - val_loss: 0.4533 - val_weighted_dice_loss: 0.4533

Epoch 00135: val_weighted_dice_loss did not improve from 0.42006
Epoch 136/150
 - 1583s - loss: 0.0416 - weighted_dice_loss: 0.0416 - val_loss: 0.4535 - val_weighted_dice_loss: 0.4535

Epoch 00136: val_weighted_dice_loss did not improve from 0.42006
Epoch 137/150
 - 1585s - loss: 0.0415 - weighted_dice_loss: 0.0415 - val_loss: 0.4608 - val_weighted_dice_loss: 0.4608

Epoch 00137: val_weighted_dice_loss did not improve from 0.42006
Epoch 138/150
 - 1591s - loss: 0.0414 - weighted_dice_loss: 0.0414 - val_loss: 0.4431 - val_weighted_dice_loss: 0.4431

Epoch 00138: val_weighted_dice_loss did not improve from 0.42006
Epoch 139/150
 - 1585s - loss: 0.0412 - weighted_dice_loss: 0.0412 - val_loss: 0.4404 - val_weighted_dice_loss: 0.4404

Epoch 00139: val_weighted_dice_loss did not improve from 0.42006
Epoch 140/150
 - 1588s - loss: 0.0410 - weighted_dice_loss: 0.0410 - val_loss: 0.4490 - val_weighted_dice_loss: 0.4490

Epoch 00140: val_weighted_dice_loss did not improve from 0.42006
Epoch 141/150
 - 1586s - loss: 0.0410 - weighted_dice_loss: 0.0410 - val_loss: 0.4457 - val_weighted_dice_loss: 0.4457

Epoch 00141: val_weighted_dice_loss did not improve from 0.42006
Epoch 142/150
 - 1585s - loss: 0.0408 - weighted_dice_loss: 0.0408 - val_loss: 0.4539 - val_weighted_dice_loss: 0.4539

Epoch 00142: val_weighted_dice_loss did not improve from 0.42006
Epoch 143/150
 - 1588s - loss: 0.0406 - weighted_dice_loss: 0.0406 - val_loss: 0.4524 - val_weighted_dice_loss: 0.4524

Epoch 00143: val_weighted_dice_loss did not improve from 0.42006
Epoch 144/150
 - 1589s - loss: 0.0405 - weighted_dice_loss: 0.0405 - val_loss: 0.4481 - val_weighted_dice_loss: 0.4481

Epoch 00144: val_weighted_dice_loss did not improve from 0.42006
Epoch 145/150
 - 1589s - loss: 0.0404 - weighted_dice_loss: 0.0404 - val_loss: 0.4409 - val_weighted_dice_loss: 0.4409

Epoch 00145: val_weighted_dice_loss did not improve from 0.42006
Epoch 146/150
 - 1588s - loss: 0.0403 - weighted_dice_loss: 0.0403 - val_loss: 0.4396 - val_weighted_dice_loss: 0.4396

Epoch 00146: val_weighted_dice_loss did not improve from 0.42006
Epoch 147/150
 - 1589s - loss: 0.0402 - weighted_dice_loss: 0.0402 - val_loss: 0.4437 - val_weighted_dice_loss: 0.4437

Epoch 00147: val_weighted_dice_loss did not improve from 0.42006
Epoch 148/150
 - 1585s - loss: 0.0401 - weighted_dice_loss: 0.0401 - val_loss: 0.4409 - val_weighted_dice_loss: 0.4409

Epoch 00148: val_weighted_dice_loss did not improve from 0.42006
Epoch 149/150
 - 1586s - loss: 0.0399 - weighted_dice_loss: 0.0399 - val_loss: 0.4511 - val_weighted_dice_loss: 0.4511

Epoch 00149: val_weighted_dice_loss did not improve from 0.42006
Epoch 150/150
 - 1584s - loss: 0.0398 - weighted_dice_loss: 0.0398 - val_loss: 0.4530 - val_weighted_dice_loss: 0.4530

Epoch 00150: val_weighted_dice_loss did not improve from 0.42006
Finished training.
